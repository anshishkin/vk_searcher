{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b49c038e-88ff-4289-b26c-17b371624923",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'core'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOSTGRES_PARSER_DATABASE_CONNECTION_URL\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpostgresql+psycopg2://vk_searcher:vk_searcher@vk_postgres/vk_searcher\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'core'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "os.environ[\"POSTGRES_PARSER_DATABASE_CONNECTION_URL\"] = 'postgresql+psycopg2://parser:parser@0.0.0.0:15432/parser'\n",
    "os.environ[\"CLICKHOUSE_PARSER_DATABASE_CONNECTION_URL\"] = 'clickhouse+native://parser:parser@0.0.0.0:9000/parser'\n",
    "from pathlib import Path\n",
    "from core import config\n",
    "from services.db_executor import DBExecutor\n",
    "from services.processor import FileProcessor\n",
    "from services.cleaner import DataFrameCleaner\n",
    "from services.ml.openai_indentificator import OpenAIIndentificator\n",
    "from services.validator import DataFrameValidator\n",
    "import pandas as pd\n",
    "import os\n",
    "import sqlparse\n",
    "\n",
    "with open(\"core/prompts_test.json\", \"r\") as json_file:\n",
    "    prompts = json.load(json_file)\n",
    "from tqdm import tqdm \n",
    "with open(\"core/templates.yaml\") as stream:\n",
    "    templates = yaml.safe_load(stream)\n",
    "\n",
    "file_processor = FileProcessor()\n",
    "file_cleaner = DataFrameCleaner()\n",
    "file_validator = DataFrameValidator(templates=templates)\n",
    "openai_indentificator = OpenAIIndentificator(config=config.OPENAIConfig,\n",
    "                                             templates=templates)\n",
    "db_executor = DBExecutor(\"postgres\")\n",
    "\n",
    "gpt_models=['o1-preview','o1-mini','gpt-4o-mini','gpt-4o']\n",
    "limit_ind_cols=[50,100,500]\n",
    "limit_ind_seg =[5,10,15,30,50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9e78b9d-78bf-4566-b004-9f7550db915a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3887beca-a2ba-4a7f-94f7-634280867818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlparse\n",
    "import collections\n",
    "import pandas as pd\n",
    "\n",
    "with open('/home/andrei/Downloads/vk_data/vk.sql', 'r') as sqldump:\n",
    "\n",
    "   parser = sqlparse.parsestream(sqldump)\n",
    "   headers = {}\n",
    "   contents = collections.defaultdict(list)\n",
    "\n",
    "   for statement in parser:\n",
    "        \n",
    "       if statement.get_type() == 'INSERT':\n",
    "\n",
    "           sublists = statement.get_sublists()\n",
    "           table_info = next(sublists)\n",
    "           table_name = table_info.get_name()\n",
    "\n",
    "           headers[table_name] = [\n",
    "            col.get_name()\n",
    "            for col in table_info.get_parameters()\n",
    "            ]\n",
    "\n",
    "           contents[table_name].extend(\n",
    "            tuple(\n",
    "                s.value.strip('\"\\'')\n",
    "                for s in next(rec.get_sublists()).get_identifiers()\n",
    "            )\n",
    "            for rec in next(sublists).get_sublists()\n",
    "            )\n",
    "           break\n",
    "            \n",
    "data = {\n",
    "    name: pd.DataFrame.from_records(table, columns = headers[name])\n",
    "    for name, table in contents.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a41843ad-2d3d-4929-80a9-a75e33d903ff",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "I/O operation on closed file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m   \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mINSERT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m       \u001b[49m\u001b[43msublists\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sublists\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.jupyterlab/lib/python3.11/site-packages/sqlparse/engine/filter_stack.py:37\u001b[0m, in \u001b[0;36mFilterStack.run\u001b[0;34m(self, sql, encoding)\u001b[0m\n\u001b[1;32m     34\u001b[0m stream \u001b[38;5;241m=\u001b[39m StatementSplitter()\u001b[38;5;241m.\u001b[39mprocess(stream)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Output: Stream processed Statements\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstmt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grouping\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstmt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgrouping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstmt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.jupyterlab/lib/python3.11/site-packages/sqlparse/engine/statement_splitter.py:87\u001b[0m, in \u001b[0;36mStatementSplitter.process\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m     84\u001b[0m EOS_TTYPE \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mWhitespace, T\u001b[38;5;241m.\u001b[39mComment\u001b[38;5;241m.\u001b[39mSingle\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Run over all stream tokens\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mttype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Yield token if we finished a statement and there's no whitespaces\u001b[39;49;00m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# It will count newline token as a non whitespace. In this context\u001b[39;49;00m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# whitespace ignores newlines.\u001b[39;49;00m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# why don't multi line comments also count?\u001b[39;49;00m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconsume_ws\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mttype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mEOS_TTYPE\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStatement\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.jupyterlab/lib/python3.11/site-packages/sqlparse/lexer.py:121\u001b[0m, in \u001b[0;36mLexer.get_tokens\u001b[0;34m(self, text, encoding)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03mReturn an iterable of (tokentype, value) pairs generated from\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m`text`. If `unfiltered` is set to `True`, the filtering mechanism\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m``stack`` is the initial stack (default: ``['root']``)\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, TextIOBase):\n\u001b[0;32m--> 121\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: I/O operation on closed file."
     ]
    }
   ],
   "source": [
    "   for statement in parser:\n",
    "\n",
    "       if statement.get_type() == 'INSERT':\n",
    "\n",
    "           sublists = statement.get_sublists()\n",
    "           table_info = next(sublists)\n",
    "           table_name = table_info.get_name()\n",
    "\n",
    "           headers[table_name] = [\n",
    "            col.get_name()\n",
    "            for col in table_info.get_parameters()\n",
    "            ]\n",
    "\n",
    "           contents[table_name].extend(\n",
    "            tuple(\n",
    "                s.value.strip('\"\\'')\n",
    "                for s in next(rec.get_sublists()).get_identifiers()\n",
    "            )\n",
    "            for rec in next(sublists).get_sublists()\n",
    "            )\n",
    "        \n",
    "\n",
    "data = {\n",
    "    name: pd.DataFrame.from_records(table, columns = headers[name])\n",
    "    for name, table in contents.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d91b93-950c-41dc-98fc-5a5a099fbb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{Path(item).stem}.csv', 'w') as f_out:\n",
    "        # write the header\n",
    "        f_out.write((','.join(columns)+'\\n'))\n",
    "        i = 0\n",
    "        with open(item, \"r\") as f_in:\n",
    "            cnt_sep = 0\n",
    "            while (b := f_in.read(1)):\n",
    "                #print(b)\n",
    "                if ord(b) == new_line[i]:\n",
    "                    # keep matching the newline block\n",
    "                    i += 1\n",
    "                    if i == len(new_line):\n",
    "                        # if matched entirely, write just a newline\n",
    "                        f_out.write('\\n')\n",
    "                        cnt_sep = 0\n",
    "                        i = 0\n",
    "                    # write nothing while matching\n",
    "                    continue\n",
    "                elif i > 0:\n",
    "                    # if you reach this, it was a partial match, write it\n",
    "                    f_out.write(new_line[:i])\n",
    "                    i = 0\n",
    "                if b == \"\\t\":\n",
    "                    if cnt_sep >=len(columns)-1:\n",
    "                        f_out.write(';')\n",
    "                    else:\n",
    "                        cnt_sep+=1\n",
    "                        f_out.write(',')\n",
    "               \n",
    "                else:\n",
    "                    # write the byte if no match\n",
    "                    f_out.write(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa5684a2-c393-4911-8479-4f3599c6ad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "root='/home/andrei/Downloads/vk_data/'\n",
    "root_cleaned='/home/andrei/Downloads/cleaned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cb1801e9-f6cc-43ee-91d0-3acbb6ac34bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vk_path= []\n",
    "for path, subdirs, files in os.walk(root):\n",
    "    if 'vk_full_001' not in path and 'unknown' not in path:\n",
    "        for name in files:\n",
    "            vk_path.append((path, name))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "654edd2f-2e0b-4aea-bf35-f9d4294a6754",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/home/andrei/Downloads/vk_data/',\n",
       "  'Сеть_магазинов_DNS,вк_парсинг_04_12_2022.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/', '1.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'VK Parsing 2023 (1).txt'),\n",
       " ('/home/andrei/Downloads/vk_data/',\n",
       "  'Школа «Летово»,вк парсинг 09.12.2022.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/', '3.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'VK Parsing 2023 (0).txt'),\n",
       " ('/home/andrei/Downloads/vk_data/', '2.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'vk_contact_phones.sql'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'HoweMine-dump VK.txt'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'Тверь,вк парсинг 05.12.2022.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'вк.txt'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'Вк.xlsx'),\n",
       " ('/home/andrei/Downloads/vk_data/',\n",
       "  'DNS_Восточная_Сибирь_вк_парсинг_04_12_2022.txt'),\n",
       " ('/home/andrei/Downloads/vk_data/',\n",
       "  'DNS Королев вк парсинг (04.12.2022).txt'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'DNS. Крым,вк парсинг 04.12.2022.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'Vk Phone_2021_5,7kk.txt'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'VK Parsing 2023 (2).txt'),\n",
       " ('/home/andrei/Downloads/vk_data/',\n",
       "  'DNS_Мурманская_область,вк_парсинг_04_12_2022.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'VK 2014 208k.txt'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'VK Parsing 2023 (5).txt'),\n",
       " ('/home/andrei/Downloads/vk_data/',\n",
       "  'ДНСDNS_Иваново_Кострома,вк_парсинг_04_12_2022.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'VK Parsing 2023 (6).txt'),\n",
       " ('/home/andrei/Downloads/vk_data/',\n",
       "  'Сеть_магазинов_DNS_Восточная_Сибирь,вк_парсинг_04_12_2022.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'VK.com.txt'),\n",
       " ('/home/andrei/Downloads/vk_data/',\n",
       "  'dns,солянка_из_пабликов,вк_парсинг_04_12_2022.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/',\n",
       "  'Сеть_магазинов_DNS_Уральский_регион,вк_парсинг_04_12_2022.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'vk _ 2023 5КК.txt'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'vk_contacts.sql'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'ADAMAS,вк парсинг 28.11.2022.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'VK_DUMPS.04.txt'),\n",
       " ('/home/andrei/Downloads/vk_data/',\n",
       "  'DNS TechnoPoint,вк парсинг 04.12.2022.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'vk.pay parsing (2022).txt'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'LorentCraft VK.txt'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'DNS Королев,вк парсинг 04.12.2022.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/',\n",
       "  'Взломанная_база_пользователей_ВК_от_Украинских_хакеров_с_любовью.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'VK MoscowScraping (2022).txt'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'VK Parsing 2023 (3).txt'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'VK Parsing 2023 (8).txt'),\n",
       " ('/home/andrei/Downloads/vk_data/',\n",
       "  'DNS  ДНС Сибирь,вк парсинг 04.12.2022.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'VK Parsing 2023 (9).txt'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'VK Parsing 2023 (4).txt'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'Ржев,вк парсинг 05.12.2022.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'VK Parsing 2023 (7).txt'),\n",
       " ('/home/andrei/Downloads/vk_data/',\n",
       "  'DNS_ДНС_Архангельская_область,вк_парсинг_04_12_2022.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'VK - 5 mln - login-pass.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'vk_contact_emails.sql'),\n",
       " ('/home/andrei/Downloads/vk_data/',\n",
       "  'DNS  ДНС Кавказ,вк парсинг 04.12.2022.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/',\n",
       "  'DNS Черноземье,вк парсинг 04.12.2022.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/',\n",
       "  'DNS_Алтайский_край_и_Алтай,вк_парсинг_04_12_2022.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'vk_full.7z'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'Really World 12.2020.VK.txt'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'DNS Омск,вк парсинг 04.12.2022.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'Парсинг Вконтакте жен. МСК.xlsx'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'vk.sql'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'vk.com.txt'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'vk.txt'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'VK_DUMPS.05_good.txt'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'Vk.pay_2020_164k.txt'),\n",
       " ('/home/andrei/Downloads/vk_data/',\n",
       "  'DNSДНС_Москва_МО_и_Тверь,вк_парсинг_04_12_2022.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'vk.com-@LegioNLeakeR.txt'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'VK_DUMPS.03.txt'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'DNS Поволжье,вк парсинг 04.12.2022.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/', 'DNS Томск,вк парсинг 04.12.2022.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/',\n",
       "  'Г_Донской,Тульская_область,вк_парсинг_25_11_2022_.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/',\n",
       "  'Подслушано_в_ДНС_сообщество_бывших_и_нынешних_работников,вк_парсинг.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/VK_128M_txt_001', 'VK_128M.txt'),\n",
       " ('/home/andrei/Downloads/vk_data/База данных Vkontakte GetContact',\n",
       "  'GetContactVK.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/vk.com_28kk_2022', 'VK_128M222.txt'),\n",
       " ('/home/andrei/Downloads/vk_data/VKCleaned', 'VK_128M222.txt'),\n",
       " ('/home/andrei/Downloads/vk_data/Vk.pay_2022_164k', 'Vk.pay_2020_164k.txt'),\n",
       " ('/home/andrei/Downloads/vk_data/GetContact_VK59M', 'GetContactVK.csv'),\n",
       " ('/home/andrei/Downloads/vk_data/VK.com DB', 'VK.com.txt'),\n",
       " ('/home/andrei/Downloads/vk_data/VK 2019 txt', 'VK 2019.txt')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vk_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0a1866-41a5-44bc-b5e6-6baf7aaeec87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "enc,sep,df = file_processor.process_csv('VK_Parsing_2023_(5).csv',nrows = 1000)\n",
    "df.head(10)\n",
    "#file_processor.process_json(document_full_path)\n",
    "#file_processor.process_sql(document_full_path)\n",
    "#file_processor.process_excel(document_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdb0c190-2df0-41ec-93b6-57b782956637",
   "metadata": {},
   "outputs": [],
   "source": [
    "vk_path= []\n",
    "root='/home/andrei/Downloads/vk_data/'\n",
    "\n",
    "for path, subdirs, files in os.walk(root):\n",
    "    if 'vk_full_001' not in path:\n",
    "        for name in files:\n",
    "           if name.endswith(\"csv\"):\n",
    "            vk_path.append((path, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "231ac5b5-2a38-4721-a43e-4cd01df8a7c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [16:24<00:00, 984.91s/it]\n"
     ]
    }
   ],
   "source": [
    "for file in tqdm(vk_path):\n",
    "    full_path = os.path.join(file[0],file[1])\n",
    "    enc,sep,df = file_processor.process_csv(full_path)\n",
    "    df = file_cleaner.get_dataframe(df)\n",
    "    columns_indentify = openai_indentificator.indentify_by_segments(prompts[\"prompts\"][\"determine_column_segments\"], df)\n",
    "    df = file_validator.process_dataframe(df, columns_indentify)\n",
    "    df.to_csv(f'{root_cleaned}/{Path(full_path).name}',header=True,index=False,sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d411d218-97f0-4891-9137-9cc4a0a40aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_full_path = '/home/andrei/Downloads/vk_data/vk.sql'\n",
    "idx=800\n",
    "#document_full_path = os.path.join(vk_path[idx][0],vk_path[idx][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67747a9b-6665-4de8-8806-69fbf9f7111a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/andrei/Downloads/vk_data/vk.sql'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_full_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a757a8-85fe-4d2f-9bf2-7d6cd1c67a10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac16880-3320-4b6b-bd0a-77034473e860",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(document_full_path, \"r\") as sql_file:\n",
    "    statements = sqlparse.split(sql_file.read())\n",
    "    full_values = []\n",
    "    for statement in statements:\n",
    "        if statement.startswith(\"INSERT INTO\"):\n",
    "            lol = sqlparse.format(statement, strip_comments=True)\n",
    "            pre, values = lol.rstrip(\";\").partition(\" VALUES \")[::2]          \n",
    "            values = [x.replace(\"'\",\"\").lstrip(\"(\").rstrip(\")\").split(\",\") for x in values.split(\",\\n\")]\n",
    "            full_values.extend(values)\n",
    "            cols = pre.replace(\"`\", \"\").partition(\"(\")[2]\n",
    "    d = {z[0]: list(z[1:]) for z in zip(cols.rstrip(\")\").split(\", \"), *list(full_values))}\n",
    "    dataset = pd.DataFrame.from_dict(d)\n",
    "    dataset.to_csv(f'/home/andrei/Downloads/out/lol.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a5baf1-5aea-4bcf-a692-ab4e701884f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a5700b82-e86c-4c26-b1a8-350e4a5866ed",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>sex</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>path_photo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Павел</td>\n",
       "      <td>Дуров</td>\n",
       "      <td>Мужской</td>\n",
       "      <td>Санкт-Петербург</td>\n",
       "      <td>Россия</td>\n",
       "      <td>https://sun1-20.userapi.com/s/v1/if1/ZNDlEasX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>Настя</td>\n",
       "      <td>Васильева</td>\n",
       "      <td>Женский</td>\n",
       "      <td>Лобня</td>\n",
       "      <td>Россия</td>\n",
       "      <td>https://sun1-47.userapi.com/s/v1/ig1/G8z0QSno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>Александр</td>\n",
       "      <td>Беспалов</td>\n",
       "      <td>Мужской</td>\n",
       "      <td>Санкт-Петербург</td>\n",
       "      <td>Россия</td>\n",
       "      <td>https://sun1-15.userapi.com/s/v1/if1/LDvcV_xh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>Илья</td>\n",
       "      <td>Турпиашвили</td>\n",
       "      <td>Мужской</td>\n",
       "      <td>Москва</td>\n",
       "      <td>Россия</td>\n",
       "      <td>https://sun1-16.userapi.com/c13/u00020/a_26d1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>Михаил</td>\n",
       "      <td>Равдоникас</td>\n",
       "      <td>Мужской</td>\n",
       "      <td>Санкт-Петербург</td>\n",
       "      <td>Россия</td>\n",
       "      <td>https://sun1-55.userapi.com/s/v1/if1/Xcs65bzf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>Семен</td>\n",
       "      <td>Воронин</td>\n",
       "      <td>Мужской</td>\n",
       "      <td>Санкт-Петербург</td>\n",
       "      <td>Россия</td>\n",
       "      <td>https://sun1-15.userapi.com/s/v1/ig2/wS98S1Va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24</td>\n",
       "      <td>Рамми</td>\n",
       "      <td>Цицуашвили</td>\n",
       "      <td>Мужской</td>\n",
       "      <td>Санкт-Петербург</td>\n",
       "      <td>Россия</td>\n",
       "      <td>https://sun1-93.userapi.com/s/v1/if2/6ZJRWbGQ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25</td>\n",
       "      <td>Анастасия</td>\n",
       "      <td>Ведущенко</td>\n",
       "      <td>Женский</td>\n",
       "      <td>Санкт-Петербург</td>\n",
       "      <td>Россия</td>\n",
       "      <td>https://sun9-28.userapi.com/c9573/u00025/a_4a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>36</td>\n",
       "      <td>Иван</td>\n",
       "      <td>Смирнов</td>\n",
       "      <td>Мужской</td>\n",
       "      <td>Москва</td>\n",
       "      <td>Россия</td>\n",
       "      <td>https://sun1-19.userapi.com/s/v1/if1/383z8_XO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>37</td>\n",
       "      <td>Михаил</td>\n",
       "      <td>Казекин</td>\n",
       "      <td>Мужской</td>\n",
       "      <td>Санкт-Петербург</td>\n",
       "      <td>Россия</td>\n",
       "      <td>https://sun1-54.userapi.com/s/v1/if1/I3ZjJ4fE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>41</td>\n",
       "      <td>Владислав</td>\n",
       "      <td>Миллер</td>\n",
       "      <td>Мужской</td>\n",
       "      <td>Москва</td>\n",
       "      <td>Россия</td>\n",
       "      <td>https://sun1-97.userapi.com/s/v1/if2/4Wt5yMhR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>46</td>\n",
       "      <td>Андрей</td>\n",
       "      <td>Лесохин</td>\n",
       "      <td>Мужской</td>\n",
       "      <td>Санкт-Петербург</td>\n",
       "      <td>Россия</td>\n",
       "      <td>https://sun1-19.userapi.com/s/v1/if1/TKrDXgEH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>52</td>\n",
       "      <td>Михаил</td>\n",
       "      <td>Вознесенский</td>\n",
       "      <td>Мужской</td>\n",
       "      <td>Санкт-Петербург</td>\n",
       "      <td>Россия</td>\n",
       "      <td>https://sun1-28.userapi.com/s/v1/ig1/TXTypXPe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>55</td>\n",
       "      <td>Андрей</td>\n",
       "      <td>Смирнов</td>\n",
       "      <td>Мужской</td>\n",
       "      <td>Москва</td>\n",
       "      <td>Россия</td>\n",
       "      <td>https://sun1-87.userapi.com/s/v1/if1/vcQAv9M_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>56</td>\n",
       "      <td>Андрей</td>\n",
       "      <td>Чернышев</td>\n",
       "      <td>Мужской</td>\n",
       "      <td>Москва</td>\n",
       "      <td>Россия</td>\n",
       "      <td>https://sun1-84.userapi.com/s/v1/if1/Awj9ndJB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>59</td>\n",
       "      <td>Анастасия</td>\n",
       "      <td>Желудкова</td>\n",
       "      <td>Женский</td>\n",
       "      <td>Москва</td>\n",
       "      <td>Россия</td>\n",
       "      <td>https://sun1-29.userapi.com/s/v1/if1/eaxmZfG5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>60</td>\n",
       "      <td>Даниэль</td>\n",
       "      <td>Розенберг</td>\n",
       "      <td>Мужской</td>\n",
       "      <td>Москва</td>\n",
       "      <td>Россия</td>\n",
       "      <td>https://sun1-29.userapi.com/s/v1/if1/Hzn0t48l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>61</td>\n",
       "      <td>Григорий</td>\n",
       "      <td>Конрадт</td>\n",
       "      <td>Мужской</td>\n",
       "      <td>London</td>\n",
       "      <td>Великобритания</td>\n",
       "      <td>https://sun9-67.userapi.com/c987/u00061/a_825...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>68</td>\n",
       "      <td>Вика</td>\n",
       "      <td>Мирилашвили</td>\n",
       "      <td>Женский</td>\n",
       "      <td>Санкт-Петербург</td>\n",
       "      <td>Россия</td>\n",
       "      <td>https://sun9-19.userapi.com/c301409/u00068/a_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>70</td>\n",
       "      <td>Митя</td>\n",
       "      <td>Миронов</td>\n",
       "      <td>Мужской</td>\n",
       "      <td>Санкт-Петербург</td>\n",
       "      <td>Россия</td>\n",
       "      <td>https://sun1-87.userapi.com/s/v1/if1/rKMoj1ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>74</td>\n",
       "      <td>Наташа</td>\n",
       "      <td>Филимонова</td>\n",
       "      <td>Женский</td>\n",
       "      <td>Санкт-Петербург</td>\n",
       "      <td>Россия</td>\n",
       "      <td>https://sun1-88.userapi.com/s/v1/if1/m9mvDda-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>75</td>\n",
       "      <td>Екатерина</td>\n",
       "      <td>Копылова</td>\n",
       "      <td>Женский</td>\n",
       "      <td>Санкт-Петербург</td>\n",
       "      <td>Россия</td>\n",
       "      <td>https://sun1-26.userapi.com/s/v1/if1/2bxKs4rS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>88</td>\n",
       "      <td>Полина</td>\n",
       "      <td>Олейник</td>\n",
       "      <td>Женский</td>\n",
       "      <td>Санкт-Петербург</td>\n",
       "      <td>Россия</td>\n",
       "      <td>https://sun1-28.userapi.com/s/v1/ig2/672hmR5a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>93</td>\n",
       "      <td>Анна</td>\n",
       "      <td>Забоева</td>\n",
       "      <td>Женский</td>\n",
       "      <td>Санкт-Петербург</td>\n",
       "      <td>Россия</td>\n",
       "      <td>https://vk.com/images/camera_400.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>94</td>\n",
       "      <td>Екатерина</td>\n",
       "      <td>Теплова</td>\n",
       "      <td>Женский</td>\n",
       "      <td>Санкт-Петербург</td>\n",
       "      <td>Россия</td>\n",
       "      <td>https://sun1-27.userapi.com/s/v1/if1/6wDc1azK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>99</td>\n",
       "      <td>Наталья</td>\n",
       "      <td>Брагина</td>\n",
       "      <td>Женский</td>\n",
       "      <td>Санкт-Петербург</td>\n",
       "      <td>Россия</td>\n",
       "      <td>https://sun1-55.userapi.com/s/v1/if1/VJmTZr5u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  first_name      last_name       sex              city  \\\n",
       "0    1       Павел          Дуров   Мужской   Санкт-Петербург   \n",
       "1    9       Настя      Васильева   Женский             Лобня   \n",
       "2   17   Александр       Беспалов   Мужской   Санкт-Петербург   \n",
       "3   20        Илья    Турпиашвили   Мужской            Москва   \n",
       "4   21      Михаил     Равдоникас   Мужской   Санкт-Петербург   \n",
       "5   22       Семен        Воронин   Мужской   Санкт-Петербург   \n",
       "6   24       Рамми     Цицуашвили   Мужской   Санкт-Петербург   \n",
       "7   25   Анастасия      Ведущенко   Женский   Санкт-Петербург   \n",
       "8   36        Иван        Смирнов   Мужской            Москва   \n",
       "9   37      Михаил        Казекин   Мужской   Санкт-Петербург   \n",
       "10  41   Владислав         Миллер   Мужской            Москва   \n",
       "11  46      Андрей        Лесохин   Мужской   Санкт-Петербург   \n",
       "12  52      Михаил   Вознесенский   Мужской   Санкт-Петербург   \n",
       "13  55      Андрей        Смирнов   Мужской            Москва   \n",
       "14  56      Андрей       Чернышев   Мужской            Москва   \n",
       "15  59   Анастасия      Желудкова   Женский            Москва   \n",
       "16  60     Даниэль      Розенберг   Мужской            Москва   \n",
       "17  61    Григорий        Конрадт   Мужской            London   \n",
       "18  68        Вика    Мирилашвили   Женский   Санкт-Петербург   \n",
       "19  70        Митя        Миронов   Мужской   Санкт-Петербург   \n",
       "20  74      Наташа     Филимонова   Женский   Санкт-Петербург   \n",
       "21  75   Екатерина       Копылова   Женский   Санкт-Петербург   \n",
       "22  88      Полина        Олейник   Женский   Санкт-Петербург   \n",
       "23  93        Анна        Забоева   Женский   Санкт-Петербург   \n",
       "24  94   Екатерина        Теплова   Женский   Санкт-Петербург   \n",
       "25  99     Наталья        Брагина   Женский   Санкт-Петербург   \n",
       "\n",
       "            country                                         path_photo  \n",
       "0            Россия   https://sun1-20.userapi.com/s/v1/if1/ZNDlEasX...  \n",
       "1            Россия   https://sun1-47.userapi.com/s/v1/ig1/G8z0QSno...  \n",
       "2            Россия   https://sun1-15.userapi.com/s/v1/if1/LDvcV_xh...  \n",
       "3            Россия   https://sun1-16.userapi.com/c13/u00020/a_26d1...  \n",
       "4            Россия   https://sun1-55.userapi.com/s/v1/if1/Xcs65bzf...  \n",
       "5            Россия   https://sun1-15.userapi.com/s/v1/ig2/wS98S1Va...  \n",
       "6            Россия   https://sun1-93.userapi.com/s/v1/if2/6ZJRWbGQ...  \n",
       "7            Россия   https://sun9-28.userapi.com/c9573/u00025/a_4a...  \n",
       "8            Россия   https://sun1-19.userapi.com/s/v1/if1/383z8_XO...  \n",
       "9            Россия   https://sun1-54.userapi.com/s/v1/if1/I3ZjJ4fE...  \n",
       "10           Россия   https://sun1-97.userapi.com/s/v1/if2/4Wt5yMhR...  \n",
       "11           Россия   https://sun1-19.userapi.com/s/v1/if1/TKrDXgEH...  \n",
       "12           Россия   https://sun1-28.userapi.com/s/v1/ig1/TXTypXPe...  \n",
       "13           Россия   https://sun1-87.userapi.com/s/v1/if1/vcQAv9M_...  \n",
       "14           Россия   https://sun1-84.userapi.com/s/v1/if1/Awj9ndJB...  \n",
       "15           Россия   https://sun1-29.userapi.com/s/v1/if1/eaxmZfG5...  \n",
       "16           Россия   https://sun1-29.userapi.com/s/v1/if1/Hzn0t48l...  \n",
       "17   Великобритания   https://sun9-67.userapi.com/c987/u00061/a_825...  \n",
       "18           Россия   https://sun9-19.userapi.com/c301409/u00068/a_...  \n",
       "19           Россия   https://sun1-87.userapi.com/s/v1/if1/rKMoj1ou...  \n",
       "20           Россия   https://sun1-88.userapi.com/s/v1/if1/m9mvDda-...  \n",
       "21           Россия   https://sun1-26.userapi.com/s/v1/if1/2bxKs4rS...  \n",
       "22           Россия   https://sun1-28.userapi.com/s/v1/ig2/672hmR5a...  \n",
       "23           Россия               https://vk.com/images/camera_400.png  \n",
       "24           Россия   https://sun1-27.userapi.com/s/v1/if1/6wDc1azK...  \n",
       "25           Россия   https://sun1-55.userapi.com/s/v1/if1/VJmTZr5u...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a94c64-248a-4d52-8cd4-45d52fa004bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import sqlparse\n",
    "import pandas as pd\n",
    "document_full_path = '/home/andrei/Downloads/vk_data/vk.sql'\n",
    "\n",
    "def read_in_chunks(file_object, chunk_size=1000000):\n",
    "    \"\"\"Lazy function (generator) to read a file piece by piece.\n",
    "    Default chunk size: 1k.\"\"\"\n",
    "    while True:\n",
    "        data = file_object.read(chunk_size)\n",
    "        if not data:\n",
    "            break\n",
    "        yield data\n",
    "\n",
    "with open(document_full_path, \"r\") as sql_file:\n",
    "    for idx, piece in tqdm(enumerate(read_in_chunks(sql_file))):\n",
    "        \n",
    "        parser = sqlparse.parsestream(piece)\n",
    "        headers = {}\n",
    "        contents = collections.defaultdict(list)\n",
    "    \n",
    "        for statement in parser:\n",
    "    \n",
    "            if statement.get_type() == 'INSERT':\n",
    "    \n",
    "               sublists = statement.get_sublists()\n",
    "               table_info = next(sublists)\n",
    "               table_name = table_info.get_name()\n",
    "            \n",
    "               headers[table_name] = [\n",
    "                col.get_name()\n",
    "                for col in table_info.get_parameters()\n",
    "                ]\n",
    "               contents[table_name].extend(\n",
    "                    tuple(\n",
    "                    s.value.strip('\"\\'')\n",
    "                    for s in next(rec.get_sublists()).get_identifiers()\n",
    "                    )\n",
    "                for rec in next(sublists).get_sublists()\n",
    "            )\n",
    "    \n",
    "        data = {\n",
    "            name: pd.DataFrame.from_records(table, columns = headers[name])\n",
    "            for name, table in contents.items()[:1]\n",
    "        }\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4984912b-1218-4bec-86bf-92ed93cef860",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35801139-549d-4c7f-8385-3248fd3471a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = file_cleaner.get_dataframe(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7722d3a-0aa5-4f40-981e-bf1821644cd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns_indentify_col = openai_indentificator.indentify_by_columns(prompts[\"prompts\"][\"determine_column_contents\"], df)\n",
    "columns_indentify_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8b6415-ad62-4f7b-9d64-13476c34ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_indentify_seg = openai_indentificator.indentify_by_segments(prompts[\"prompts\"][\"determine_column_segments\"], df)\n",
    "columns_indentify_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7638eb1-6ab4-4c40-a08b-3a21c87a5e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_indentify_summary= openai_indentificator.indentify_summary(prompts[\"prompts\"][\"determine_summary\"], df)\n",
    "columns_indentify_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a22690-a626-48b6-90b3-34d4980de559",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = file_validator.process_dataframe(df, columns_indentify_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3449d0-a3cb-429e-bd35-c35515114960",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c460a79-d89a-462c-9959-a39a9af2d0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked(seq, chunksize):\n",
    "    \"\"\"Yields items from an iterator in chunks.\"\"\"\n",
    "    it = iter(seq)\n",
    "    while True:\n",
    "        yield chain([it.next()], islice(it, chunksize-1))\n",
    "\n",
    "for chunk in chunked(records, 1000):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d656b2-0c21-42e5-93ee-53ca0279de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{root_cleaned}/{Path(document_full_path).name}',header=True,index=False,sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "500ddabf-ac70-4dd1-96be-b2232c4f4253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfd3cd9e-8d8e-4452-9e68-2971d3de45f5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x87 in position 50: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/andrei/Downloads/vk_data/VK Parsing 2023 (5).txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.jupyterlab/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.jupyterlab/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.jupyterlab/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.jupyterlab/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.jupyterlab/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:663\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x87 in position 50: invalid start byte"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv(f'/home/andrei/Downloads/vk_data/VK Parsing 2023 (5).txt',header=0,index_col=False,sep=\",\",nrows=1000,low_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "361a62f6-d027-42c9-a921-64f76872249a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FullName\\tCountry\\tCity\\tGender\\tVkID\\tBDay\\tFollowers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7\\t\\t\\t\\t\\t\\t\\t\\t\\t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hhy Yh \\t\\t\\t2\\t659128102\\t14.3.1999\\t0\\t\\t\\t\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hhy Yhg \\tKyrgyzstan\\tBishkek\\t2\\t564813550\\t5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hhy Yhh \\t\\t\\t2\\t548501322\\t5.5.1987\\t0\\t\\t\\t\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hhy Yhyu \\t\\t\\t2\\t644504700\\t7.5.1996\\t0\\t\\t\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Hhzhz Hzjsjsjdgz \\t\\t\\t1\\t561197059\\t1.2.2005\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Hhzhz Jajz \\tRussia\\tSaint Petersburg\\t2\\t5051...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Hhzhz Jjjsj \\t\\t\\t2\\t631982296\\t7.8.1993\\t0\\t\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Hhzhz Nsjs \\t\\t\\t1\\t462681787\\t3.1.2002\\t0\\t\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Hhzhzh Dhjdjdj \\tThailand\\tBangkok\\t2\\t6574492...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    FullName\\tCountry\\tCity\\tGender\\tVkID\\tBDay\\tFollowers\n",
       "0                                  7\\t\\t\\t\\t\\t\\t\\t\\t\\t    \n",
       "1    Hhy Yh \\t\\t\\t2\\t659128102\\t14.3.1999\\t0\\t\\t\\t\\...    \n",
       "2    Hhy Yhg \\tKyrgyzstan\\tBishkek\\t2\\t564813550\\t5...    \n",
       "3    Hhy Yhh \\t\\t\\t2\\t548501322\\t5.5.1987\\t0\\t\\t\\t\\...    \n",
       "4    Hhy Yhyu \\t\\t\\t2\\t644504700\\t7.5.1996\\t0\\t\\t\\t...    \n",
       "..                                                 ...    \n",
       "995  Hhzhz Hzjsjsjdgz \\t\\t\\t1\\t561197059\\t1.2.2005\\...    \n",
       "996  Hhzhz Jajz \\tRussia\\tSaint Petersburg\\t2\\t5051...    \n",
       "997  Hhzhz Jjjsj \\t\\t\\t2\\t631982296\\t7.8.1993\\t0\\t\\...    \n",
       "998  Hhzhz Nsjs \\t\\t\\t1\\t462681787\\t3.1.2002\\t0\\t\\t...    \n",
       "999  Hhzhzh Dhjdjdj \\tThailand\\tBangkok\\t2\\t6574492...    \n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ec81605-2fb7-4293-a546-3e94d0111856",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vkontakte_id</th>\n",
       "      <th>phone_number</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>email_address</th>\n",
       "      <th>password_cleartext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18485939</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Esrtydfghmailru</td>\n",
       "      <td>Esrtydfghmailru</td>\n",
       "      <td>esrtydfgh@mail.ru</td>\n",
       "      <td>esrtydfgh@mail.ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18485940</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ро</td>\n",
       "      <td>Пр</td>\n",
       "      <td>medinceva.svetlana@yandex.ru</td>\n",
       "      <td>2222222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18485941</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Олег</td>\n",
       "      <td>Стащенюк</td>\n",
       "      <td>oleg_stashenyuk@mail.ru</td>\n",
       "      <td>OLEG1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18485942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Svetlana</td>\n",
       "      <td>Sizaya</td>\n",
       "      <td>luchezarepocel@mail.ru</td>\n",
       "      <td>eqIFaRAxYhY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18485943</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ирка</td>\n",
       "      <td>Барина</td>\n",
       "      <td>vitka_97_97@mail.ru</td>\n",
       "      <td>310797v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>18598574</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Комил</td>\n",
       "      <td>Раззоков</td>\n",
       "      <td>komilrazzokov@inbox.ru</td>\n",
       "      <td>10011985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>18598575</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Александр</td>\n",
       "      <td>Атрашонак</td>\n",
       "      <td>a.alexxander.s@mail.ru</td>\n",
       "      <td>as1602802059ea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>18598576</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Оля</td>\n",
       "      <td>Бабкина</td>\n",
       "      <td>olga-babkina-1981@mail.ru</td>\n",
       "      <td>15061981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>18598577</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Саня</td>\n",
       "      <td>Лихачев</td>\n",
       "      <td>travakur@mael.ru</td>\n",
       "      <td>sany_07rash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>18598578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Рита</td>\n",
       "      <td>Мельниченко</td>\n",
       "      <td>MarGo320@mail.ru</td>\n",
       "      <td>q19091991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       vkontakte_id  phone_number       first_name        last_name  \\\n",
       "0          18485939           NaN  Esrtydfghmailru  Esrtydfghmailru   \n",
       "1          18485940           NaN               Ро               Пр   \n",
       "2          18485941           NaN             Олег         Стащенюк   \n",
       "3          18485942           NaN         Svetlana           Sizaya   \n",
       "4          18485943           NaN             Ирка           Барина   \n",
       "...             ...           ...              ...              ...   \n",
       "99995      18598574           NaN            Комил         Раззоков   \n",
       "99996      18598575           NaN        Александр        Атрашонак   \n",
       "99997      18598576           NaN              Оля          Бабкина   \n",
       "99998      18598577           NaN             Саня          Лихачев   \n",
       "99999      18598578           NaN             Рита      Мельниченко   \n",
       "\n",
       "                      email_address password_cleartext  \n",
       "0                 esrtydfgh@mail.ru  esrtydfgh@mail.ru  \n",
       "1      medinceva.svetlana@yandex.ru         2222222222  \n",
       "2           oleg_stashenyuk@mail.ru           OLEG1995  \n",
       "3            luchezarepocel@mail.ru        eqIFaRAxYhY  \n",
       "4               vitka_97_97@mail.ru            310797v  \n",
       "...                             ...                ...  \n",
       "99995        komilrazzokov@inbox.ru           10011985  \n",
       "99996        a.alexxander.s@mail.ru     as1602802059ea  \n",
       "99997     olga-babkina-1981@mail.ru           15061981  \n",
       "99998              travakur@mael.ru        sany_07rash  \n",
       "99999              MarGo320@mail.ru          q19091991  \n",
       "\n",
       "[100000 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "   def format_sql(self, file_path: str):\n",
    "        full_values = []\n",
    "        with open(file_path, \"r\") as sql_file:\n",
    "            statements = sqlparse.split(sql_file.read())\n",
    "            for statement in statements:\n",
    "                if statement.startswith(\"INSERT INTO\"):\n",
    "                    lol = sqlparse.format(statement.lower(), strip_comments=True)\n",
    "                    pre, values = lol.rstrip(\";\").partition(\" values\\n\")[::2]\n",
    "                    values = [x.lstrip(\"(\").rstrip(\")\").split(\", \") for x in values.split(\",\\n\")]\n",
    "                    full_values.extend(values)\n",
    "                    cols = pre.replace(\"`\", \"\").partition(\"(\")[2]\n",
    "            d = {z[0]: list(z[1:]) for z in zip(cols.rstrip(\")\").split(\", \"), *list(full_values))}\n",
    "            return d\n",
    "\n",
    "    def process_sql(\n",
    "        self,\n",
    "        file_path: str,\n",
    "    ):\n",
    "        logger.info(f\"Starting SQL file processing for: {file_path}\")\n",
    "        try:\n",
    "            encoding = self.detect_encoder(file_path)\n",
    "            logger.info(f\"Using detected encode: {encoding}\")\n",
    "            if encoding:\n",
    "                dataset = pd.DataFrame.from_dict(self.format_sql(file_path))\n",
    "                logger.info(f\"Successfully loaded SQL into DataFrame. Shape: {dataset.shape}\")\n",
    "                # print(encoding, separator, dataset.shape)\n",
    "                return encoding, None, dataset\n",
    "            else:\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a71d9211-8e1a-42af-ae37-acb95fdf5b6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdb_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Project/IDX/telegram_bot/services/db_executor.py:37\u001b[0m, in \u001b[0;36mDBExecutor.execute\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     35\u001b[0m standard_entities \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m---> 37\u001b[0m     standard_entities\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_entity_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_st\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     38\u001b[0m     cnt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     39\u001b[0m session\u001b[38;5;241m.\u001b[39madd_all(standard_entities)\n",
      "File \u001b[0;32m~/Project/IDX/telegram_bot/services/db_executor.py:70\u001b[0m, in \u001b[0;36mDBExecutor._parse_entity_and_save\u001b[0;34m(self, row, columns, col_st)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m xs:\n\u001b[1;32m     69\u001b[0m         kek \u001b[38;5;241m=\u001b[39m kek \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=None, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28meval\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself.StandardEvent(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkek\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     uuid_ \u001b[38;5;241m=\u001b[39m uuid\u001b[38;5;241m.\u001b[39muuid4()\n",
      "File \u001b[0;32m<string>:0\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "db_executor.execute(df.iloc[:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723d014f-40cb-44fc-8645-4246f9872c53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65018fa-f8e8-4238-9ece-b6346b624299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.config import OPENAIConfig\n",
    "import json\n",
    "with open(\"core/prompts_test.json\", \"r\") as json_file:\n",
    "    prompts = json.load(json_file)\n",
    "\n",
    "prompt_col = prompts[\"prompts\"][\"determine_column_contents\"]\n",
    "prompt_seg = prompts[\"prompts\"][\"determine_column_segments\"]\n",
    "idx_col = 1\n",
    "limit = 100\n",
    "content_col = ( df[~df.iloc[:, idx_col].isnull()]\n",
    "                .reset_index(drop=True)\n",
    "                .iloc[: limit, idx_col]\n",
    "                .to_json(orient=\"records\",force_ascii=False)\n",
    "            )\n",
    "start_idx = 0 if idx_col - 2 < 0 else idx_col - 2\n",
    "#prompt_seg = prompt_seg.replace(\"DataN\", str(df.columns[idx_col]))\n",
    "content_seg = json.dumps( \n",
    "                df[~df.iloc[:, idx_col].isnull()]\n",
    "                .reset_index(drop=True)\n",
    "                .iloc[: limit, start_idx : idx_col +2]\n",
    "                .to_dict(\"list\") , ensure_ascii=False\n",
    "            )\n",
    "prompt_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4143ebef-530e-415d-acf2-b08b1526e97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b825b65-ce29-4f1a-992b-287f1f6de4bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_columns = OpenAIIndentificator(OPENAIConfig).indentify_by_columns(prompts[\"prompts\"][\"determine_column_contents\"],df)\n",
    "pos_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75a1065-186b-49e5-8f00-0738723e95af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "OpenAIIndentificator(OPENAIConfig).indentify_by_columns(prompts[\"prompts\"][\"determine_column_segments\"],df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daa5c2f-7684-49fd-b455-463c298294b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_columns=['full_name', 'phone_number', 'email_address', 'snils_rf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7af7e5d-4bf9-4d3f-b588-c7f1ac5e4097",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/documents/db/8b90d2b54f8a488581d005f5901728a2.csv',dtype=str,sep=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7dbd72-46c3-4750-bd0a-de61d00eb415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4964ddf9-c9e0-4514-8649-923d3807e67f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OPENAIConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgetpass\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mOPENAIConfig\u001b[49m\u001b[38;5;241m.\u001b[39mapi_key\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OPENAIConfig' is not defined"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAIConfig.api_key\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model=OPENAIConfig.model,\n",
    "    streaming=True,\n",
    "    openai_proxy=OPENAIConfig.proxy_url\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ab859d-9870-4cc7-879c-980f0aeb864f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088edfba-bf28-4922-a944-effb7f2c6ac8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", prompt_col),\n",
    "    (\"user\", content_col)\n",
    "])\n",
    "\n",
    "ll = prompt_template.invoke({\"possible_columns\": ' '.join(col_st)})\n",
    "model(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e921fcba-b7f6-4cf4-a34e-499cc11b7d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_seg = \"\"\"Please read the data in the dictionary below. Your goal is to determine what data is contained in the columns: {column}, and select the most appropriate option from the list that I give below, and then output it. If none of the suggested options matches the data, just print \"unknown\". When identifying, consider the data surrounding the {column} in the dictionary, including any dependencies or relationships between columns. In any case, do not output anything other than \\\"unknown\\\" or the names of the fields that I provide. If I don't provide any data, answer \\\"unknown\\\". Let's get started. Your goal is to answer whether the data I provide in the following message is as follows: {possible_columns}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d9c99e-689f-4143-aba7-40bbabaef6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_all = \"\"\"Please read the data in the dictionary below. Your goal is to determine what data is contained in the columns: {column},\\\n",
    "    and select the most appropriate option from the list that I give below, and then write it into the dictionary in the format column: option from the list. \\\n",
    "    If none of the suggested options matches the data, just write column:\"unknown\". When identifying, consider the data surrounding the in the dictionary,\\\n",
    "    including any dependencies or relationships between columns. In any case, do not output anything other than \"unknown\" or the names of the fields that I provide. \\\n",
    "    If I don't provide any data, answer \"unknown\". \n",
    "    Let's get started. Your goal is to answer whether the data I provide in the following message is as follows: {possible_columns}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee0c35e-c629-4e92-abd2-46588a2cc17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name=[\"0\",\"1\",\"2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9619add2-b89c-474e-9bcf-6cbf0634cf72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "prompt = (\n",
    "    PromptTemplate.from_template(prompt_all)\n",
    ")\n",
    "prompt = prompt.format(column = ', '.join([f\"{str(x)}\" for x in col_name]) ,possible_columns=', '.join(col_st))\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc8c91e-0dfa-4add-915d-a63066b5d633",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=prompt),\n",
    "    HumanMessage(content=content_col),\n",
    "]\n",
    "\n",
    "model(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295a1231-bf1d-4665-8fab-81c3a79d59fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "chain = model | parser\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d644a-8907-4f48-abfa-ec5fee94a6f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce02aacf-b910-472c-a2bf-08ebfe7d3aee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873f255a-d8e5-4f02-9869-59ea6f2c20dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import os\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "class ProxyClient(httpx.Client):\n",
    "    def __deepcopy__(self, memo):\n",
    "        return self\n",
    "\n",
    "llm_config = {\n",
    "    \"config_list\": [\n",
    "    {\n",
    "        \"model\": config.OPENAIConfig.model,\n",
    "        \"api_key\": config.OPENAIConfig.api_key,\n",
    "        \"http_client\": ProxyClient(proxy=config.OPENAIConfig.proxy_url),\n",
    "        \"temperature\": config.OPENAIConfig.temperature\n",
    "    }\n",
    "],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1592b30-547b-4584-8d25-9948ae6dc167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4543a6f3-ed8b-4230-80a1-49016a98989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_col = 6\n",
    "limit = 100\n",
    "content_col = ( df[~df.iloc[:, idx_col].isnull()]\n",
    "                .reset_index(drop=True)\n",
    "                .iloc[: limit, idx_col]\n",
    "                .to_json(orient=\"records\",force_ascii=False)\n",
    "            )\n",
    "start_idx = 0 if idx_col - 2 < 0 else idx_col - 2\n",
    "prompt_seg = prompt_seg.replace(\"DataN\", str(df.columns[idx_col]))\n",
    "content_seg = json.dumps( \n",
    "                df[~df.iloc[:, idx_col].isnull()]\n",
    "                .reset_index(drop=True)\n",
    "                .iloc[: limit, start_idx : idx_col +2]\n",
    "                .to_dict(\"list\") , ensure_ascii=False\n",
    "            )\n",
    "prompt_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56da16f7-ab32-4ff6-9bfb-8f85f9d61ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, TypedDict\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "# Define the tools for the agent to use\n",
    "@tool\n",
    "def search(query: str):\n",
    "    \"\"\"Call to surf the web.\"\"\"\n",
    "    # This is a placeholder, but don't tell the LLM that...\n",
    "    if \"sf\" in query.lower() or \"san francisco\" in query.lower():\n",
    "        return \"It's 60 degrees and foggy.\"\n",
    "    return \"It's 90 degrees and sunny.\"\n",
    "\n",
    "\n",
    "tools = [search]\n",
    "\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "model = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0).bind_tools(tools)\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return END\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge(\"tools\", 'agent')\n",
    "\n",
    "# Initialize memory to persist state between graph runs\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable.\n",
    "# Note that we're (optionally) passing the memory when compiling the graph\n",
    "app = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Use the Runnable\n",
    "final_state = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]},\n",
    "    config={\"configurable\": {\"thread_id\": 42}}\n",
    ")\n",
    "final_state[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72642bd-2e3e-44ed-935b-999187fae30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_col = [{\"role\": \"system\", \"content\": prompt_col}, {\"role\": \"user\", \"content\": content_col}]\n",
    "messages_seg = [{\"role\": \"system\", \"content\": prompt_seg}, {\"role\": \"user\", \"content\": content_seg}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e269c60-e982-4d45-bed7-86085c1caade",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [{\"role\": \"user\", \"content\": content_col}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db6563d-0cea-45fd-bd41-fa31cb33b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_col = ConversableAgent(\n",
    "    \"an_col\",\n",
    "    system_message=prompt_col,\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    ")\n",
    "\n",
    "agent_seg = ConversableAgent(\n",
    "    \"an_seg\",\n",
    "    system_message=prompt_seg,\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9dddb8-b124-495e-86ab-32866b67f14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = agent_col.generate_reply(messages=messages_col)\n",
    "print(reply)\n",
    "reply = agent_seg.generate_reply(messages=messages_seg)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064532c8-348b-4ed6-b68a-fbe0449d298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    \"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False,\n",
    "    default_auto_reply=\"\",\n",
    "    is_termination_msg=lambda x: True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb8577d-afc1-452f-bccd-4fb3d622eed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_test= \"Your goal is to determine what data is in the 'FATHER' column and choose the most appropriate option prompt_test = Please examine the data in the dictionary below. Your goal is to determine what data is in the 'FATHER' column and choose the most appropriate option from the list I provide below, and then output it. If none of the provided options fit the data, simply output 'unknown'. When identifying, consider the data surrounding 'FATHER' in the dictionary, including any dependencies or relationships between columns. Absolutely never output anything except 'unknown' or the field names I provide. If I provide no data, reply 'unknown'. Let's start. Your goal is to answer \\\n",
    "if the data I provide in the next message is a: phone_number email_address russian_passport_number_rf snils_rf inn_rf street_address state province region country first_name last_name username password_cleartext password_hash unknown_hash hash_salt ipv4_address ipv6_address full_name telegram_id last_login_date registration_date access_date date_of_birth year: A sample of data can only be a year if all samples provided are greater than 1900 and less than 2100. month day gender: Might be in the form of a letter, e.g., F, M. Might be present in different languages.- patronymic related_person foreign_passport_rf driver_license_rf oms_policy_rf military_id_rf pension_certificate_rf birth_certificate_rf residence_permit_rf temporary_residence_permit_rf migration_card_rf labor_book_rf tax_certificate_rf student_id_rf work_permit_rf marital_status place_of_birth nationality ethnicity social_security_number bank_account_number credit_card_number credit_card_cvv credit_card_date passport_expiry_date employment_status employer job_title income education_level degree social_media_profile website vkontakte_id github_id linkedin_id skype_id fax_number emergency_contact_name emergency_contact_phone emergency_contact_relationship insurance_policy_number- insurance_provider vehicle_registration_number license_plate_number loyalty_card_number blood_type legal_cases shopping_history pet_name \\\n",
    " Please examine the data in the dictionary : \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeb555f-ad7a-451a-ba51-8faba059320a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = user_proxy.initiate_chat(agent_col, message = content_col, max_turns=4)\n",
    "result.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3d43cc-ba24-4cb2-968c-2338d9e02d4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = user_proxy.initiate_chat(agent_seg, message = content_seg, max_turns=1)\n",
    "result.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e4b478-b5f0-47a8-99bf-74a7b4b669b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_results = number_agent.initiate_chats(\n",
    "    [\n",
    "        {\n",
    "            \"recipient\": user_proxy,\n",
    "            \"message\": ,\n",
    "            \"max_turns\": 1,\n",
    "            \"summary_method\": \"last_msg\",\n",
    "        },\n",
    "        {\n",
    "            \"recipient\": multiplier_agent,\n",
    "            \"message\": \"These are my numbers\",\n",
    "            \"max_turns\": 1,\n",
    "            \"summary_method\": \"last_msg\",\n",
    "        },\n",
    "         {\n",
    "            \"recipient\": multiplier_agent,\n",
    "            \"message\": \"These are my numbers\",\n",
    "            \"max_turns\": 1,\n",
    "            \"summary_method\": \"last_msg\",\n",
    "        },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa2672b-9c78-4ba7-b015-c5ba65f65ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_test =  [{\"role\": \"user\", \"content\": prompt_test + content_seg}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0270430-91e5-406b-9baa-7a443d22a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabb4e46-75f3-486c-a608-187bf1710389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from autogen.agentchat.contrib.capabilities.text_compressors import LLMLingua\n",
    "from autogen.agentchat.contrib.capabilities.transforms import TextMessageCompressor\n",
    "import autogen\n",
    "from autogen.agentchat.contrib.capabilities import transform_messages\n",
    "\n",
    "system_message = \"You are a world class researcher.\"\n",
    "#config_list = [{\"model\": \"gpt-4-turbo\", \"api_key\": os.getenv(\"OPENAI_API_KEY\")}]\n",
    "\n",
    "# Define your agent; the user proxy and an assistant\n",
    "researcher_col = autogen.ConversableAgent(\n",
    "    \"researcher_col\",\n",
    "    llm_config=llm_config,\n",
    "    max_consecutive_auto_reply=1,\n",
    "    system_message=system_message,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").find(\"TERMINATE\") >= 0,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "researcher_seg = autogen.ConversableAgent(\n",
    "    \"researcher_seg\",\n",
    "    llm_config=llm_config,\n",
    "    max_consecutive_auto_reply=1,\n",
    "    system_message=system_message,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").find(\"TERMINATE\") >= 0,\n",
    "\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "researcher_full = autogen.ConversableAgent(\n",
    "    \"researcher_full\",\n",
    "    llm_config=llm_config,\n",
    "    max_consecutive_auto_reply=1,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").find(\"TERMINATE\") >= 0,\n",
    "\n",
    "    system_message=system_message,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    \"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False,\n",
    "    default_auto_reply=\"\",\n",
    "    is_termination_msg=lambda x: True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1834274-c526-4e95-ac2c-33a8e3a9d57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupchat = autogen.GroupChat(\n",
    "    agents=[researcher_seg, researcher_full,researcher_col],\n",
    "    messages=[],\n",
    "    speaker_selection_method=\"round_robin\",  # With two agents, this is equivalent to a 1:1 conversation.\n",
    "    allow_repeat_speaker=False,\n",
    "    max_round=1,\n",
    ")\n",
    "\n",
    "manager = autogen.GroupChatManager(\n",
    "    groupchat=groupchat,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").find(\"TERMINATE\") >= 0,\n",
    "    llm_config=llm_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb2bcfc-b756-46f3-8fc0-548d35cd56c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_col = ConversableAgent(\n",
    "    \"an_col\",\n",
    "    system_message=prompt_col,\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    ")\n",
    "\n",
    "agent_seg = ConversableAgent(\n",
    "    \"an_seg\",\n",
    "    system_message=prompt_seg,\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7fc439-2a6d-4a1f-8704-685784aeb290",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e687a7-a6a2-4a47-a0b5-98327e1ac5b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "groupchat = autogen.GroupChat(\n",
    "    agents=[agent_col, agent_seg],\n",
    "    messages=[],\n",
    "    speaker_selection_method=\"round_robin\",  # With two agents, this is equivalent to a 1:1 conversation.\n",
    "    allow_repeat_speaker=False,\n",
    "    max_round=6,\n",
    ")\n",
    "\n",
    "manager = autogen.GroupChatManager(\n",
    "    groupchat=groupchat,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").find(\"TERMINATE\") >= 0,\n",
    "    llm_config=llm_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f52d679-52fe-4d6f-91cb-7bf60e3a511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_test =  [{\"role\": \"user\", \"content\": prompt_test + content_seg}]\n",
    "summary_prompt = \"Give me back the most frequent answer. If not, then output 'unknown' \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69645bf7-2fa5-46d6-bd87-c0ce9132840e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nested_chats =  [\n",
    "        {\n",
    "            \"recipient\": manager,\n",
    "            \"message\": content_col,\n",
    "            \"summary_method\":\"reflection_with_llm\",\n",
    "            #\"summary_prompt\": \"Give me back the most frequent answer. If not, then output 'unknouwn' \"\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b58385-9696-4538-93a0-6916f5e8b564",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy.register_nested_chats(\n",
    "    nested_chats,\n",
    "    trigger=lambda sender: sender not in [manager],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478206b4-603f-4665-93fd-0a2bd48fcb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy.generate_reply(messages=content_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082e360b-1d19-4850-8092-f8edc4c99041",
   "metadata": {},
   "outputs": [],
   "source": [
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6796ab00-9045-429f-b816-30d2aa49eb16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_proxy.initiate_chat(manager, \n",
    "                         message = content_col, \n",
    "                         max_turns=1,\n",
    "                         summary_method = \"reflection_with_llm\",\n",
    "                         summary_prompt = summary_prompt\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672e68e3-6a86-438b-9031-c26b668d7820",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"On which days in 2024 was Microsoft Stock higher than $370?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0ad249-eae3-418c-ae5c-e1488342d171",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "llm_lingua = LLMLingua()\n",
    "text_compressor = TextMessageCompressor(text_compressor=llm_lingua)\n",
    "compressed_text = text_compressor.apply_transform([{\"content\": content_seg}])\n",
    "print(text_compressor.get_logs([], []))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720cc36f-9932-43e5-8c35-3c6a3ab14972",
   "metadata": {},
   "outputs": [],
   "source": [
    "#context_handling = transform_messages.TransformMessages(transforms=[text_compressor])\n",
    "#context_handling.add_to_agent(researcher)\n",
    "\n",
    "#message = \"Summarize this research paper for me, include the important information\" + pdf_text\n",
    "result = user_proxy.initiate_chat(recipient=researcher, clear_history=True, message=prompt_test + content_seg, silent=True)\n",
    "\n",
    "print(result.chat_history[1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe6d103-403a-47c1-9dbd-3de6d3d5aade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b5ae5d-f29d-49e1-84e6-feaf0e70b26b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ec8937-4836-4288-b322-37cf8f0d38f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ef6342-641f-4a9f-a2e8-f7789b88d440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683de18f-d008-4541-90b1-4db8da5cb63b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1d574f-707d-4cd8-a123-e914b020c4cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04345dc4-2d0d-4147-b3e1-db5f70f4883e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e6a611-1a4d-41cd-89d2-c387fdb861c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c36b5c-8ec7-4538-b54f-f000e59209f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b6c35d-f803-4d9f-aa9f-d0da6e6c31aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6f6065-a638-4835-84b7-a0d413b67aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4773d6-bcd3-4fdb-be8d-065208836afd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae631911-d986-4145-a5b6-30e795ab1413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b1b769-1f59-4895-a82e-a9573aab032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5623d98c-2967-4677-b050-97fc9d5a133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame([[0, np.NaN, 0], [0, np.NaN, 0], [0, np.NaN, np.NaN], [0, np.NaN, 0], [0, np.NaN, 0],[np.NaN, np.NaN, np.NaN]], columns=['lol','kek','tt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a299e8-6690-4f16-88fb-dc20672bd603",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd797321-ad20-4bad-82e2-95dea5d0be63",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bd4bdb-c2a3-4e0e-a573-894909cb1c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 600\n",
    "pd.options.display.max_colwidth = 400\n",
    "import ru_core_news_md\n",
    "nlp = ru_core_news_md.load()\n",
    "nlp = spacy.load('ru_core_news_md',parser=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1e0a42-42fd-48cd-819b-be2e8ffcb518",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('test/mini-db6.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89f0131-9533-457e-a3de-d74b5200cf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840521e4-7e45-45cf-b88f-7e247476c28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs = [row for row in df.itertuples()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eacd058-e859-48f2-9cab-fa22328ed714",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'test/test.txt'\n",
    "text = open(filepath, encoding='utf-8').read()\n",
    "document = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f94a6-0bf9-4137-9f3d-beda239d6595",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc = nlp('БелыйДаниилОлегович, 0675679082, daniel.bely@gmail.com')\n",
    "for token in doc.ents:\n",
    "   print(token.text, token.label_,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e738c5a-5b13-4ce4-9f12-9b84d0dd0805",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1483c017-ff48-4ac9-92b2-5fc0cf294ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities=[]\n",
    "import tqdm\n",
    "for i in df.iloc[:,1].tolist():\n",
    "    doc = nlp(i)\n",
    "    for entity in doc.ents:\n",
    "        entities.append((i, entity.text, entity.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24996b96-dea6-46bc-98a4-926d4da6987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d4334b-e2b2-4a09-bf5b-af5b787cea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2ca6d0-4f61-4a2c-be3b-396869dc278a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[d for d in nlp.pipe(df.iloc[:,2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029e6365-a04a-40f4-bf9d-a5adc804c353",
   "metadata": {},
   "outputs": [],
   "source": [
    "for named_entity in document.ents:\n",
    "    print(named_entity, named_entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0508db6b-00e1-4424-ab67-7b67b72fd776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Union\n",
    "import pandas as pd\n",
    "import logging\n",
    "import csv\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataFrameCleaner:\n",
    "    model_name =\"dataframe_cleaner\"\n",
    "\n",
    "    def get_dataframe(self, df: pd.DataFrame,\n",
    "                        limit_na: float = .) -> pd.DataFrame :\n",
    "        cleaned_df = self._process_clean(df,limit_na)\n",
    "        #save_to_df(dataset, f'documents/cleaned/test.csv')\n",
    "        return cleaned_df.shape\n",
    "\n",
    "    def drop_na(self, df: pd.DataFrame,limit_na: float) -> pd.DataFrame :\n",
    "        df = df.dropna(how='all',axis = 0).dropna(thresh=limit_na, axis=1)\n",
    "        return df\n",
    "    \n",
    "    def drop_nonunique_values(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        drop_cols = [c for c in list(df) if df[c].nunique() <= 1]\n",
    "        return df.drop(columns=drop_cols)\n",
    "    \n",
    "    def drop_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return df.drop_duplicates(keep='last')\n",
    "\n",
    "    def _process_clean( self, df: pd.DataFrame, limit_na: float) -> pd.DataFrame :\n",
    "        df_notna = self.drop_na(df,limit_na)\n",
    "        df_nonunique_values = self.drop_nonunique_values(df_notna)\n",
    "        print(df_nonunique_values.shape)\n",
    "        cleaned_df = self.drop_duplicates(df_nonunique_values)\n",
    "        print(cleaned_df.shape)\n",
    "        return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035bc25f-62f5-4c89-833c-2f3b9fdc51c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('test/db2.csv',header=0,encoding='SHIFT_JIS',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b6d456-6a12-4bd3-a5ed-a1f9dcee4214",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrameCleaner().get_dataframe(df = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d053e6-a0bb-45b4-bba8-a9d7a477f877",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd06b623-d2bc-4386-b0db-7696e833849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().mean()<0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a991d28d-3437-4fdd-8430-d925a599c603",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.iloc[:, df.isnull().mean() > 0.67 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a87508f-d3ae-4bd5-837e-f98de6fec784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Union\n",
    "\n",
    "import logging\n",
    "import vt\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class VirusTotalHandler:\n",
    "\n",
    "    def __init__(self, api_key: str):\n",
    "\n",
    "        try:\n",
    "            self.client = vt.Client(api_key)\n",
    "            logger.info(\"VirusTotal client initialized successfully\")\n",
    "        except ValueError as e:\n",
    "            logger.error(f\"Invalid API key format: {e}\")\n",
    "        except ConnectionError as e:\n",
    "            logger.error(f\"Network connection error while initializing VirusTotal client: {e}\")\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Unexpected error occurred while initializing VirusTotal client: {e}\")\n",
    "    \n",
    "    def get_status(self,file_path: str) -> bool:\n",
    "        return self.analyze_if_malicious(file_path)\n",
    "    \n",
    "    def analyze_if_malicious(self, file_path: str) -> Union[bool, None]:\n",
    "        \"\"\"\n",
    "        Analyze a file to determine if it's potentially malicious.\n",
    "\n",
    "        :param file_path: Path to the file to be analyzed.\n",
    "        :return: True if the file is potentially malicious, False if it's not, None if analysis couldn't be completed.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Starting analysis for file: {file_path}\")\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            logger.error(f\"File not found: {file_path}\")\n",
    "            raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "\n",
    "        # Compute SHA256 hash\n",
    "        try:\n",
    "            sha256_hash = compute_sha256(file_path)\n",
    "            logger.debug(f\"Computed SHA256 hash for {file_path}: {sha256_hash}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to compute SHA256 hash for {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            # Check if the file has been analyzed before\n",
    "            logger.info(f\"Checking VirusTotal for existing analysis of {sha256_hash}\")\n",
    "            file_report = self.client.get_object(f\"/files/{sha256_hash}\")\n",
    "            last_analysis_results = file_report.last_analysis_results\n",
    "\n",
    "            is_malicious = any(\n",
    "                result['category'] in ['malicious', 'suspicious'] for result in last_analysis_results.values())\n",
    "            logger.info(\n",
    "                f\"Analysis result for {file_path}: {'Potentially malicious' if is_malicious else 'Not malicious'}\")\n",
    "            return is_malicious\n",
    "\n",
    "        except vt.error.APIError as e:\n",
    "            if e.code == \"NotFoundError\":\n",
    "                logger.info(f\"No existing analysis found for {file_path}. Initiating new scan.\")\n",
    "                return self._scan_file(file_path)\n",
    "            else:\n",
    "                logger.error(f\"VirusTotal API error occurred while analyzing {file_path}: {e}\")\n",
    "                raise\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Unexpected error occurred while analyzing {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _scan_file(self, file_path: str) -> Union[bool, None]:       \n",
    "        logger.info(f\"Initiating new scan for file: {file_path}\")\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"rb\") as file:\n",
    "                analysis = self.client.scan_file(file)\n",
    "            logger.debug(f\"Scan initiated for {file_path}. Analysis ID: {analysis.id}\")\n",
    "\n",
    "            start_time = time.time()\n",
    "            while True:\n",
    "                analysis = self.client.get_object(\"/analyses/{}\", analysis.id)\n",
    "                if analysis.status == \"completed\":\n",
    "                    logger.info(f\"Scan completed for {file_path}. Analysis ID: {analysis.id}\")\n",
    "                    break\n",
    "                elif time.time() - start_time > 300:  # 5 minutes timeout\n",
    "                    logger.warning(f\"Scan timed out for {file_path}. Analysis ID: {analysis.id}\")\n",
    "                    return None\n",
    "                time.sleep(10)  # Wait for 10 seconds before checking again\n",
    "\n",
    "            results = analysis.stats\n",
    "            is_malicious = results.get(\"malicious\", 0) > 0 or results.get(\"suspicious\", 0) > 0\n",
    "            logger.info(f\"Scan result for {file_path}: {'Potentially malicious' if is_malicious else 'Not malicious'}\")\n",
    "            logger.debug(f\"Detailed scan results for {file_path}: {results}\")\n",
    "            return is_malicious\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"An error occurred while scanning {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def __del__(self):\n",
    "        logger.info(\"Closing VirusTotal client\")\n",
    "        try:\n",
    "            self.client.close()\n",
    "            logger.debug(\"VirusTotal client closed successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error occurred while closing VirusTotal client: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af88b82-0ff4-4921-b38f-6d873e77e108",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_analysis_handler = HybridAnalysisHandler(api_key=\"g0ql1u7a696e97f51vxysq8rfddeaf1ax3jmokj63f10d50dlxx78kok656675c9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2af6e-b582-4b8d-8792-c0bf97d78698",
   "metadata": {},
   "outputs": [],
   "source": [
    "virustotal_handler = VirusTotalHandler(api_key='80f17a826d2284ae11e469602e0d070af6a7c0a39bd63968a4209351e5dd297f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b218bc-c00f-48e4-a84b-9a07a514ac05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hybrid_analysis_handler.get_status('/home/andreis/Projects/IDX/telegram_bot/test/documents/db1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724e5cfd-6b70-4f3f-b4e7-1c42a355cec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "virustotal_handler.get_status('/home/andreis/Projects/IDX/telegram_bot/test/documents/db1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88e13b6-d931-4c73-b223-58d66b835e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Union\n",
    "import chardet\n",
    "import pandas as pd\n",
    "import logging\n",
    "import csv\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CSVProcessor:\n",
    "    ANALYSE_X_FIRST_LINES = 10\n",
    "\n",
    "    def __init__(self,analyze_first_x_lines: int = None):\n",
    "        self.analyze_first_x_lines = analyze_first_x_lines or self.ANALYSE_X_FIRST_LINES\n",
    "\n",
    "    def detect_encode(self, file_path:str) -> str:\n",
    "        with open(file_path,'rb') as f:\n",
    "            data = f.read(self.analyze_first_x_lines)\n",
    "            encoding = chardet.detect(data).get(\"encoding\")\n",
    "            logger.info(f\"Detected encode: '{encoding}'\")\n",
    "            print(encoding)\n",
    "            return encoding\n",
    "\n",
    "    def get_delimiter(self , file_path, bytes = 4096):\n",
    "        sniffer = csv.Sniffer()\n",
    "        data = open(file_path, \"r\").reaimport sys\n",
    "from typing import Union\n",
    "import chardet\n",
    "import pandas as pd\n",
    "import logging\n",
    "import csv\n",
    "from .base import ProcessorHandler\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CSVProcessor(ProcessorHandler):\n",
    "    model_name =\"CSV_Processor\"\n",
    "    ANALYSE_X_FIRST_LINES = 10\n",
    "\n",
    "    def __init__(self,analyze_first_x_lines: int = None, encoding: list = None):\n",
    "        self.analyze_first_x_lines = analyze_first_x_lines or self.ANALYSE_X_FIRST_LINES\n",
    "        self.encoding = encoding\n",
    "    \n",
    "    def detect_encode(self, file_path:str) -> str:\n",
    "        with open(file_path,'rb') as f:\n",
    "            data = f.read(self.analyze_first_x_lines)\n",
    "            encoding = chardet.detect(data).get(\"encoding\")\n",
    "            logger.info(f\"Detected encode: '{encoding}'\")\n",
    "            print(encoding)\n",
    "            return encoding\n",
    "\n",
    "    def detect_delimiter(self, file_path: str) -> str:\n",
    "        with open(file_path, 'r') as csvfile:\n",
    "            delimiter = str(csv.Sniffer().sniff(csvfile.read()).delimiter)\n",
    "            print(delimiter)\n",
    "            return delimiter\n",
    "\n",
    "    def detect_separator_statistically(self, file_path:str, encoding:str):\n",
    "\n",
    "        possible_separators = [',', '\\t', ';', '|', ':', '~', '^', '||', '/', '\\\\', '#', '$', '&']\n",
    "        separator_counts = [0] * len(possible_separators)\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding = encoding) as file:\n",
    "                for _ in range(self.analyze_first_x_lines):\n",
    "                    line = file.readline()\n",
    "                    if not line:\n",
    "                        break\n",
    "                    for i, separator in enumerate(possible_separators):\n",
    "                        count = line.count(separator)\n",
    "                        separator_counts[i] += count\n",
    "\n",
    "            if all(count == 0 for count in separator_counts):\n",
    "                logger.warning(\"No separators detected statistically\")\n",
    "                return \n",
    "\n",
    "            max_count_index = separator_counts.index(max(separator_counts))\n",
    "            detected_separator = possible_separators[max_count_index]\n",
    "            logger.info(f\"Statistically detected separator: '{detected_separator}'\")\n",
    "            return detected_separator\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error during statistical separator detection: {e}\")\n",
    "            return\n",
    "\n",
    "    def get_status(self,file_path:str):\n",
    "        encoding, separator, shape = self._process_file(file_path)\n",
    "        if shape:\n",
    "            return True, encoding, separator, shape\n",
    "        return False, None, None, None\n",
    "    \n",
    "    def _process_file(self,file_path:str):\n",
    "        logger.info(f\"Starting CSV file processing for: {file_path}\")\n",
    "\n",
    "        try:\n",
    "            chardet_encoding = self.detect_encode(file_path)\n",
    "            logger.info(f\"Using detected encode: {chardet_encoding}\")\n",
    "            #separator = self.detect_delimiter(file_path)\n",
    "            #separator = self.get_delimiter(file_path)\n",
    "            #logger.info(f\"Using detected separator: '{separator}'\")\n",
    "            dataset = None\n",
    "            #if chardet_encoding:\n",
    "            encoding = self.encoding[::-1]\n",
    "            for encoder in encoding:\n",
    "                try:\n",
    "                    separator = self.detect_separator_statistically(file_path, encoder)\n",
    "                    dataset = pd.read_csv(file_path, delimiter=separator, encoding=encoder,nrows=self.analyze_first_x_lines )\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "            if dataset:\n",
    "                dataset = pd.read_csv(file_path, delimiter=separator, encoding=encoder)\n",
    "                logger.info(f\"Successfully loaded CSV into DataFrame. Shape: {dataset.shape}\")\n",
    "                print(encoding, separator, dataset.shape)\n",
    "                return encoding, separator, dataset.shape\n",
    "            else:\n",
    "                return           \n",
    "\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error occurred while processing CSV file: {e}\")\n",
    "            raised(bytes)\n",
    "        delimiter = sniffer.sniff(data).delimiter\n",
    "        return delimiter\n",
    "    \n",
    "    def detect_delimiter(self, file_path: str) -> str:\n",
    "        with open(file_path, 'r') as csvfile:\n",
    "            delimiter = str(csv.Sniffer().sniff(csvfile.read()).delimiter)\n",
    "            print(delimiter)\n",
    "            return delimiter\n",
    "\n",
    "    def detect_separator_statistically(self, file_path:str, encoding:str):\n",
    "\n",
    "        possible_separators = [',', '\\t', ';', '|', ':', '~', '^', '||', '/', '\\\\', '#', '$', '&']\n",
    "        separator_counts = [0] * len(possible_separators)\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding = encoding) as file:\n",
    "                for _ in range(self.analyze_first_x_lines):\n",
    "                    line = file.readline()\n",
    "                    if not line:\n",
    "                        break\n",
    "                    for i, separator in enumerate(possible_separators):\n",
    "                        count = line.count(separator)\n",
    "                        separator_counts[i] += count\n",
    "\n",
    "            if all(count == 0 for count in separator_counts):\n",
    "                logger.warning(\"No separators detected statistically\")\n",
    "                return \n",
    "\n",
    "            max_count_index = separator_counts.index(max(separator_counts))\n",
    "            detected_separator = possible_separators[max_count_index]\n",
    "            logger.info(f\"Statistically detected separator: '{detected_separator}'\")\n",
    "            return detected_separator\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error during statistical separator detection: {e}\")\n",
    "            return\n",
    "\n",
    "    def get_status(self,file_path:str):\n",
    "        encoding, separator, shape = self._process_file(file_path)\n",
    "        if shape:\n",
    "            return True, encoding, separator, shape\n",
    "        return False, None, None, None\n",
    "    \n",
    "    def _process_file(self,file_path:str):\n",
    "        logger.info(f\"Starting CSV file processing for: {file_path}\")\n",
    "\n",
    "        try:\n",
    "            encoding = self.detect_encode(file_path)\n",
    "            logger.info(f\"Using detected encode: '{encoding}'\")\n",
    "            #separator = self.detect_delimiter(file_path)\n",
    "            #separator = self.get_delimiter(file_path)\n",
    "            #logger.info(f\"Using detected separator: '{separator}'\")\n",
    "            if encoding :\n",
    "                try:\n",
    "                    separator = self.detect_separator_statistically(file_path, encoding)\n",
    "                    dataset = pd.read_csv(file_path, delimiter=separator, encoding=encoding)\n",
    "                except:\n",
    "                    try:\n",
    "                         separator = self.detect_separator_statistically(file_path, 'utf-8')\n",
    "                         dataset = pd.read_csv(file_path, delimiter=separator, encoding='utf-8')\n",
    "                    except:\n",
    "                        separator ';'\n",
    "                        dataset = pd.read_csv(file_path, delimiter=';', encoding='unicode_escape')\n",
    "                logger.info(f\"Successfully loaded CSV into DataFrame. Shape: {dataset.shape}\")\n",
    "                print(encoding, separator, dataset.shape)\n",
    "                return encoding, separator, dataset.shape\n",
    "            else:\n",
    "                return           \n",
    "\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error occurred while processing CSV file: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4bfb70-4496-480f-9afa-59673215f09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e350fa6-3165-42c6-8e0a-e0f02f4de033",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_processor = CSVProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02911a4d-743c-4d51-9c5f-52019b5a7c66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csv_processor.get_status('/home/andreis/Projects/IDX/telegram_bot/test/documents/db2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64642ca9-726d-423f-b6cf-650bacd07a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pandas import json_normalize\n",
    "\n",
    "def _findnestedlist(js):\n",
    "    for i in js.keys():\n",
    "        if isinstance(js[i],list):\n",
    "            return js[i]\n",
    "    for v in js.values():\n",
    "        if isinstance(v,dict):\n",
    "            return check_list(v)\n",
    "\n",
    "def _recursive_lookup(k, d):\n",
    "    if k in d:\n",
    "        return d[k]\n",
    "    for v in d.values():\n",
    "        if isinstance(v, dict):\n",
    "            return _recursive_lookup(k, v)\n",
    "    return None\n",
    "\n",
    "def flat_json(content,key):\n",
    "    nested_list = []\n",
    "    with open(content) as jsonfile:\n",
    "        js = json.loads(jsonfile.read())\n",
    "        if key is None or key == '':\n",
    "            nested_list = _findnestedlist(js)\n",
    "        else:\n",
    "            nested_list = _recursive_lookup(key, js[0])\n",
    "            return json_normalize(nested_list,sep=\"_\")\n",
    "\n",
    "key = \"documnets\" # If you don't have it, give it None\n",
    "csv_data = flat_json('Projects/IDX/telegram_bot/test/base-style.json',key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed40fa08-4b45-4c0c-bf9f-6fe7060be3be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "json_normalize('Projects/IDX/telegram_bot/test/base-style.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0329fab8-4e8d-404e-ab85-8d764e9186d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Projects/IDX/telegram_bot/test/data_doc.json') as jsonfile:\n",
    "    js = json.loads(jsonfile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924b031d-e01b-46d0-925c-fbf8504c9ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_normalize(js).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232a8654-544f-4074-89a4-c404f0715088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlparse\n",
    "with open('Projects/IDX/telegram_bot/test/tula_lk_b2b.sql', 'r') as sql_file:\n",
    "    sqlFile = sql_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb950e36-f59c-4975-87bf-3e1c6457887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlparse\n",
    "from sqlparse.sql import IdentifierList, Identifier\n",
    "from sqlparse.tokens import Keyword, DML\n",
    "\n",
    "\n",
    "def is_subselect(parsed):\n",
    "    if not parsed.is_group:\n",
    "        return False\n",
    "    for item in parsed.tokens:\n",
    "        if item.ttype is DML and item.value.upper() == 'SELECT':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def extract_from_part(parsed):\n",
    "    from_seen = False\n",
    "    for item in parsed.tokens:\n",
    "        if from_seen:\n",
    "            if is_subselect(item):\n",
    "                yield from extract_from_part(item)\n",
    "            elif item.ttype is Keyword:\n",
    "                return\n",
    "            else:\n",
    "                yield item\n",
    "        elif item.ttype is Keyword and item.value.upper() == 'FROM':\n",
    "            from_seen = True\n",
    "\n",
    "\n",
    "def extract_table_identifiers(token_stream):\n",
    "    for item in token_stream:\n",
    "        if isinstance(item, IdentifierList):\n",
    "            for identifier in item.get_identifiers():\n",
    "                yield identifier.get_name()\n",
    "        elif isinstance(item, Identifier):\n",
    "            yield item.get_name()\n",
    "        # It's a bug to check for Keyword here, but in the example\n",
    "        # above some tables names are identified as keywords...\n",
    "        elif item.ttype is Keyword:\n",
    "            yield item.value\n",
    "\n",
    "\n",
    "def extract_tables(sql):\n",
    "    stream = extract_from_part(sqlparse.parse(sql)[0])\n",
    "    return list(extract_table_identifiers(stream))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sql = \"\"\"\n",
    "    select K.a,K.b from (select H.b from (select G.c from (select F.d from\n",
    "    (select E.e from A, B, C, D, E), F), G), H), I, J, K order by 1,2;\n",
    "    \"\"\"\n",
    "\n",
    "    tables = ', '.join(extract_tables(sql))\n",
    "    print('Tables: {}'.format(tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b607ab5-b529-4b61-8e60-ec644b5bdf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables(sql):\n",
    "    \"\"\"Extract the table names from an SQL statment.\n",
    "    Returns a list of (schema, table, alias) tuples\n",
    "    \"\"\"\n",
    "    parsed = sqlparse.parse(sql)\n",
    "    if not parsed:\n",
    "        return []\n",
    "\n",
    "    # INSERT statements must stop looking for tables at the sign of first\n",
    "    # Punctuation. eg: INSERT INTO abc (col1, col2) VALUES (1, 2)\n",
    "    # abc is the table name, but if we don't stop at the first lparen, then\n",
    "    # we'll identify abc, col1 and col2 as table names.\n",
    "    insert_stmt = parsed[0].token_first().value.lower() == \"insert\"\n",
    "    stream = extract_from_part(parsed[0], stop_at_punctuation=insert_stmt)\n",
    "    return list(extract_table_identifiers(stream))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f00cd95-a75d-488d-8423-d979b21314cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlparse\n",
    "with open('Projects/IDX/telegram_bot/test/tula_lk_b2b.sql', 'r') as sql_file:\n",
    "    statements = sqlparse.split(sql_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d327ea7-cdff-4a17-af79-4546c11076b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = sqlparse.parse(sqlFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48299f5b-7e2e-4927-a869-ed1e9fa90243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcefa97e-4619-44ae-8f4a-27fb8a20d9d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_values = []\n",
    "for statement in statements:\n",
    "  if statement.startswith(\"INSERT INTO\"):\n",
    "    lol = sqlparse.format(statement.lower(), strip_comments=True)\n",
    "    pre, values = lol.rstrip(';').partition(' values\\n')[::2]\n",
    "    values = [x.lstrip('(').rstrip(')').split(', ') for x in values.split(',\\n')]\n",
    "    full_values.extend(values)\n",
    "    cols = pre.replace('`','').partition('(')[2]\n",
    "d = {z[0]: list(z[1:]) for z in zip(cols.rstrip(')').split(', '), *list(full_values))}\n",
    "len(full_values)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0235bc6-5662-462b-9f5a-2783b640848d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " pd.DataFrame.from_dict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608be0bc-8e28-4a84-bae8-1ccd191aa3ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc6376f-6d2f-4594-8200-7187321d8226",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pre, value = lol.rstrip(';').partition(' values\\n')[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72626701-61aa-4f05-9259-ef12cbbeb041",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d257307-cb86-4108-bd7f-655bfb1f67bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lol = \"insert into `b_user` (`id`, `timestamp_x`, `login`, `password`, `checkword`, `active`, `name`, `last_name`, `email`, `last_login`, `date_register`, `lid`, `personal_profession`, `personal_www`, `personal_icq`, `personal_gender`, `personal_birthdate`, `personal_photo`, `personal_phone`, `personal_fax`, `personal_mobile`, `personal_pager`, `personal_street`, `personal_mailbox`, `personal_city`, `personal_state`, `personal_zip`, `personal_country`, `personal_notes`, `work_company`, `work_department`, `work_position`, `work_www`, `work_phone`, `work_fax`, `work_pager`, `work_street`, `work_mailbox`, `work_city`, `work_state`, `work_zip`, `work_country`, `work_profile`, `work_logo`, `work_notes`, `admin_notes`, `stored_hash`, `xml_id`, `personal_birthday`, `external_auth_id`, `checkword_time`, `second_name`, `confirm_code`, `login_attempts`, `last_activity_date`, `auto_time_zone`, `time_zone`, `time_zone_offset`, `title`, `bx_user_id`, `language_id`) values \n",
    "(1,\t'2022-04-15 10:35:23',\t'admin',\t'=8baj6[f5871f89d6f189077eea08f2923bbad53',\t'l1ueqm2c261016d748eb1de5c53ff50aadd48173',\t'y',\t'имя тест',\t'фамилия',\t'pavel@ac-bizon.ru',\t'2022-05-15 17:23:19',\t'2015-08-26 16:50:39',\t's1',\t'',\t'',\t'',\t'',\tnull,\tnull,\t'',\t'',\t'',\t'',\t'',\t'',\t'',\t'',\t'',\t'0',\t'',\t'',\t'',\t'',\t'',\t'',\t'',\t'',\t'',\t'',\t'',\t'',\t'',\t'0',\t'',\tnull,\t'',\t'',\tnull,\t'1#admin#фамилия имя отчество',\tnull,\tnull,\t'2021-12-03 06:17:58',\t'отчество',\t'',\t0,\t'2020-06-05 16:41:11',\t'y',\t'selected',\t14400,\t'',\t'3e4dfb773a017ef9b13f15c7219a084e',\t''),\n",
    "(15,\t'2021-12-06 01:28:36',\t'8-950-263-4462',\t'9bmp3sj4ae898162ece725187b8c1ed32ac5e4b3',\t'ameasr5beaa3e612a7c280f11a137d1cf292696d',\t'y',\tnull,\tnull,\t'nomail@bitrix.local',\t'2012-10-17 08:14:34',\t'2012-10-17 08:13:04',\t's1',\tnull,\tnull,\tnull,\t'',\tnull,\tnull,\tnull,\tnull,\t'8-950-263-4462',\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\t'2012-10-17 08:14:34',\tnull,\t'',\t0,\t'2012-10-17 08:26:06',\t'',\tnull,\tnull,\tnull,\tnull,\tnull),\n",
    "(16,\t'2021-12-02 01:29:38',\t'8-903-046-0854',\t'\\\\1xsm1w\\\\48c44845899ced9c3dc1fd8478002026',\t'fncztaw46496bed9b2c324134587f0b7ca056f11',\t'y',\t'денис',\t'жарков',\t'imunilat@mail.ru',\t'2018-11-26 12:43:19',\t'2012-10-17 12:32:24',\t's1',\tnull,\tnull,\t'',\t'm',\tnull,\tnull,\t'',\tnull,\t'8-903-046-0854',\tnull,\t'',\tnull,\t'',\t'',\t'',\t'0',\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\t'0',\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\tnull,\t'2012-10-17 13:13:50',\t'юрьевич',\t'',\t0,\t'2014-09-02 13:46:24',\t'',\tnull,\t0,\tnull,\t'0a777c5ae843d0ded29dc4dff12a3842',\tnull);\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bea3f8-41ac-4096-b0f8-fd70202d0e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfae5a36-5ee3-46a3-af74-69def737df11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import csv\n",
    "#stmt = \"insert into `b_user` (`id`, `timestamp_x`, `login`) values (1,\t'2022-04-15 10:35:23',\t'admin'), (1,\t'2022-04-15 10:35:23',\t'admin');\"\n",
    "stmt = \"insert into table (col1, col2, col3) values (100, '() string with parantheses ()', 2.3), (100, '() string with parantheses ()', 2.3);\"\n",
    "pre, values = stmt.rstrip(';').partition(' values ')[::2]\n",
    "cols = pre.partition('(')[2]\n",
    "d = dict(zip(cols.rstrip(')').split(', '), ast.literal_eval(values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc908895-ab37-4437-9141-f1fe2f451599",
   "metadata": {},
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbf216d-3655-4b3b-be77-329d101746e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_part(parsed):\n",
    "    from_seen = False\n",
    "    for item in parsed.tokens:\n",
    "        if from_seen:\n",
    "            if is_subselect(item):\n",
    "                yield from extract_from_part(item)\n",
    "            elif item.ttype is Keyword:\n",
    "                return\n",
    "            else:\n",
    "                yield item\n",
    "        elif item.ttype is Keyword and item.value.upper() == 'FROM':\n",
    "            from_seen = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dca51a-7a74-47f1-8e03-388d472746f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a866b6b8-3c80-4b1f-9495-206ef4fe4034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6737ba-09dc-4d7e-bbaa-3fff6b638eac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087962f7-e2fd-46b1-b0cb-e4318b2abf40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
